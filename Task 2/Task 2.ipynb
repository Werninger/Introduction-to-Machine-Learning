{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d8289e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data\n",
      "(900, 11)\n",
      "2700\n",
      "<bound method NDFrame._add_numeric_operations.<locals>.max of 0      3\n",
      "1      3\n",
      "2      3\n",
      "3      3\n",
      "4      3\n",
      "      ..\n",
      "895    3\n",
      "896    3\n",
      "897    3\n",
      "898    3\n",
      "899    3\n",
      "Length: 900, dtype: int64>\n",
      "[-3.0068564  -2.858772   -2.918718   -2.7979836  -1.3434502  -1.0761555\n",
      "  1.1553545  -0.8506837  -0.4288469   0.40301594  0.8011815   2.0647051\n",
      "  3.1891823   3.366945    2.3130054   1.9745752   2.01099     1.4661626\n",
      "  2.1034665   1.5077587   3.594775    3.8479419   3.3088171   2.8572178\n",
      "  2.9606562   4.0249257   6.263088    8.056987    8.485607    9.186927\n",
      "  8.82988     8.553162    8.157765    7.643731    7.869851    7.965272\n",
      "  7.1436505   7.5689454   8.063204    7.496813    7.8920484   7.674539\n",
      "  7.5197043   8.2170315   7.846694    7.948244    7.255376    7.9147577\n",
      "  7.7807565   7.705129    8.12854     8.480243    8.460199    8.473308\n",
      "  8.207515    9.090643    8.564186    8.030152    7.794072    7.1555595\n",
      "  6.445611    6.251153    5.474255    5.074155    5.6037364   5.134621\n",
      "  5.256598    4.114028    4.3283324   4.6006374   4.438894    5.172822\n",
      "  4.562296    4.9917297   6.257561    5.765429    6.820642    8.308173\n",
      "  8.356374    8.326435    8.486842    8.660182    8.208935    7.9903746\n",
      "  8.561656    9.016581    8.707608    8.741901    7.4618273   7.6924615\n",
      "  7.8510327   7.7567825   8.860631    8.776082    9.235564    8.5505085\n",
      "  8.676716    8.000052    6.9315777   6.031298  ]\n",
      "\n",
      "Results file successfully generated!\n"
     ]
    }
   ],
   "source": [
    "# This serves as a template which will guide you through the implementation of this task.  It is advised\n",
    "# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n",
    "# First, we import necessary libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def iterativeImp(X):\n",
    "    max_iter=20\n",
    "    tol=0.0001\n",
    "    initial_strategy='median'\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    imp = IterativeImputer(max_iter=max_iter,tol=tol,initial_strategy=initial_strategy)\n",
    "    imp.fit(X)\n",
    "    IterativeImputer(random_state=0)\n",
    "    return imp.transform(X)\n",
    "\n",
    "def data_loading():\n",
    "    \"\"\"\n",
    "    This function loads the training and test data, preprocesses it, removes the NaN values and interpolates the missing \n",
    "    data using imputation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train: matrix of floats, training input with features\n",
    "    y_train: array of floats, training output with labels\n",
    "    X_test: matrix of floats: dim = (100, ?), test input with features\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    #print(\"Training data:\")\n",
    "    #print(\"Shape:\", train_df.shape)\n",
    "    #print(train_df.head(2))\n",
    "    #print('\\n')\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    #print(\"Test data:\")\n",
    "    #print(test_df.shape)\n",
    "    #print(test_df.head(2))\n",
    "\n",
    "    # Dummy initialization of the X_train, X_test and y_train   \n",
    "    X_train = np.zeros_like(train_df.drop(['price_CHF'],axis=1))\n",
    "    y_train = np.zeros_like(train_df['price_CHF'])\n",
    "    X_test = np.zeros_like(test_df)\n",
    "    \n",
    "    # TODO: Perform data preprocessing, imputation and extract X_train, y_train and X_test\n",
    "    #would it make sense to 1st do test and train and then train with chf alone?\n",
    "    #or group by season?\n",
    "    \n",
    "    \n",
    "    #1st extract y values from train_df => y, train  AND create test\n",
    "    y = train_df['price_CHF'].to_numpy()\n",
    "        #preprocess on all data!?\n",
    "    train_df = train_df.drop(['price_CHF'],axis=1) \n",
    "    \n",
    "    #2nd put put X_train and X_test together => data\n",
    "    data = [train_df, test_df]\n",
    "    data = pd.concat(data)\n",
    "    \n",
    "    #3rd onehotencoding & imputation on data\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "    data_season = ohe.fit_transform(data[['season']]).toarray()\n",
    "    X = np.append(data_season,data.drop(['season'],axis=1).to_numpy(),1)\n",
    "    \n",
    "    X = iterativeImp(X)   \n",
    "    \n",
    "        #now we can delete the price_CHF column, such as we never have used it\n",
    "    #X = np.delete(X,5,1)\n",
    "    \n",
    "    #4th split data into test and train\n",
    "    size = X_train.shape[0]\n",
    "    train = X[:size,:]\n",
    "    X_test = X[size:,:]\n",
    "    \n",
    "    #5th add y column to X_train\n",
    "    X_train = np.insert(train,5,y,1)\n",
    "    \n",
    "    #6th delete y=nan from X_train\n",
    "    X_train = X_train[~np.isnan(X_train).any(axis=1)]\n",
    "    \n",
    "    #7th get X_train, y_train\n",
    "    y_train = X_train[:,5]\n",
    "    X_train = np.delete(X_train,5,1)\n",
    "        \n",
    "    assert (X_train.shape[1] == X_test.shape[1]), \"Invalid data shape:X_train X_test\"\n",
    "    assert  (X_train.shape[0] == y_train.shape[0]), \"Invalid data shape:X_train y_train\"\n",
    "    assert (X_test.shape[0] == 100), \"Invalid data shape:X_test\"\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def new_model(X_train,y_train):\n",
    "    \n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    model = XGBRegressor(random_state = 1002)\n",
    "    search_space = {\n",
    "        \"n_estimators\" : [100,200,500],\n",
    "        \"max_depth\" : [3,6,9],\n",
    "        \"gamma\" : [0.01, 0.1],\n",
    "        \"learning_rate\" : [0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    GS = GridSearchCV( estimator = model, param_grid = search_space, scoring = [\"r2\",\"neg_root_mean_squared_error\"], refit = \"r2\", cv = 5, verbose = 4\n",
    "    )\n",
    "    GS.fit(X_train, y_train)\n",
    "    print(GS.best_estimator_)\n",
    "    print(GS.best_params_)\n",
    "    \n",
    "def modeling_and_prediction(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    This function defines the model, fits training data and then does the prediction with the test data \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: matrix of floats, training input with 10 features\n",
    "    y_train: array of floats, training output\n",
    "    X_test: matrix of floats: dim = (100, ?), test input with 10 features\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y_test: array of floats: dim = (100,), predictions on test set\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred=np.zeros(X_test.shape[0])\n",
    "    #TODO: Define the model and fit it using training data. Then, use test data to make predictions\n",
    "    alpha = 1e-10\n",
    "    n_restarts_optimizer = 0\n",
    "    #model\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern, RationalQuadratic, PairwiseKernel\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    new_model(X_train,y_train)\n",
    "    \n",
    "    model = XGBRegressor(n_estimators = 200, max_depth = 3, learning_rate = 0.1, gamma =0.01, random_state = 1002)\n",
    "    #'gamma': 0.01, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(y_pred)\n",
    "    assert y_pred.shape == (100,), \"Invalid data shape\"\n",
    "    return y_pred\n",
    "\n",
    "# Main function. You don't have to change this\n",
    "if __name__ == \"__main__\":\n",
    "    # Data loading\n",
    "    X_train, y_train, X_test = data_loading()\n",
    "    # The function retrieving optimal LR parameters\n",
    "    y_pred=modeling_and_prediction(X_train, y_train, X_test)\n",
    "    # Save results in the required format\n",
    "    dt = pd.DataFrame(y_pred) \n",
    "    dt.columns = ['price_CHF']\n",
    "    dt.to_csv('results.csv', index=False)\n",
    "    print(\"\\nResults file successfully generated!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075565a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
